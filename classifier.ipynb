{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvish7/traffic_sign_classifier/blob/master/classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67B6AzKicn8w",
        "colab_type": "text"
      },
      "source": [
        "All the necessary imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRUJQLVnan00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "from google.colab import files\n",
        "import io\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_6CEoB-djsr",
        "colab_type": "text"
      },
      "source": [
        "Fetching the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ww9l2K-lauZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(training_data_file, testing_data_file):\n",
        "    with open(training_data_file, mode='rb') as f:\n",
        "        train_data = pickle.load(f)\n",
        "    with open(testing_data_file, mode='rb') as f:\n",
        "        test_data = pickle.load(f)\n",
        "    return train, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3I-pEbIeuT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "data_path=\"/content/drive/My Drive/traffic_sign_classifier_data/\"\n",
        "\n",
        "training_data_file= open(data_path+'train.p','rb')\n",
        "test_data_file= open(data_path+'test.p','rb')\n",
        "valid_data_file=open(data_path+'valid.p', 'rb')\n",
        "\n",
        "train_data = pickle.load(training_data_file)\n",
        "test_data = pickle.load(test_data_file)\n",
        "valid_data = pickle.load(valid_data_file)\n",
        "\n",
        "X_train, y_train = train_data['features'], train_data['labels']\n",
        "X_test, y_test = test_data['features'], test_data['labels']\n",
        "X_valid,y_valid = valid_data['features'], valid_data['labels']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMleizhllBDO",
        "colab_type": "text"
      },
      "source": [
        "Pickled data is a dict having 4 key-values paris, namely\n",
        "features, labels, size and coords\n",
        "\n",
        "problem with coords --- it considers original image and pickled data has 32*32 downsampled image (this ate my head all the way)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtNmF4LSr9Zb",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Getting to know the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEjaKHq_iJMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Number of examples\n",
        "n_train, n_test = X_train.shape[0], X_test.shape[0]\n",
        "\n",
        "# What's the shape of an traffic sign image?\n",
        "image_shape = X_train[0].shape\n",
        "\n",
        "# How many classes?\n",
        "n_classes = np.unique(y_train).shape[0]\n",
        "\n",
        "print(\"Number of training examples =\", n_train)\n",
        "print(\"Number of testing examples  =\", n_test)\n",
        "print(\"Image data shape  =\", image_shape)\n",
        "print(\"Number of classes =\", n_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50a_TF6c1Xl5",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Visualize some samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyMFl28g1W64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# showing a random sample from each class\n",
        "rows, cols = 5, 9\n",
        "fig, ax_array = plt.subplots(rows, cols)\n",
        "plt.suptitle('RANDOM SAMPLES FROM TRAINING SET (one for each class)')\n",
        "\n",
        "\n",
        "for class_idx, ax in enumerate(ax_array.ravel()):\n",
        "    if class_idx < n_classes:\n",
        "        # show a random image of the current class\n",
        "        cur_X = X_train[y_train == class_idx]\n",
        "        cur_img = cur_X[np.random.randint(len(cur_X))]\n",
        "        ax.imshow(cur_img)\n",
        "        ax.set_title('{:02d}'.format(class_idx))\n",
        "    else:\n",
        "        ax.axis('off')\n",
        "      \n",
        "plt.draw()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwzQZ1-1ylJ0",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Preprocessing the data\n",
        "doing the normalization (image centering to zero image and dividing by std_div)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t2hGpWCuF1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_preprocess(X):\n",
        "\n",
        "\n",
        "    X = np.float32(X)\n",
        "\n",
        "    # standardize features\n",
        "    X -= np.mean(X)\n",
        "    X /= (np.std(X) + np.finfo('float32').eps)\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "X_train = np.array([data_preprocess(img) for img in X_train])\n",
        "\n",
        "X_test = np.array([data_preprocess(img) for img in X_test])\n",
        "\n",
        "X_valid = np.array([data_preprocess(img) for img in X_valid])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWvwkGVOe7Zh",
        "colab_type": "text"
      },
      "source": [
        "Defining model architecture\n",
        "LeNet 5 which is taught in Udacity course is used here\n",
        "paper of a model related to same problem is at http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-oBg64Gg8Qy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_pass(x):\n",
        "    \"\"\"Perform a forward pass through the network\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    x : Tensor\n",
        "        the input data\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    Tensor\n",
        "        the logits\n",
        "        \n",
        "    \"\"\"\n",
        "    # Arguments used for tf.truncated_normal --> used to randomly define the weights and biases\n",
        "    mu = 0\n",
        "    sigma = 0.1\n",
        "\n",
        "    # Layer 1: Convolutional. Input = 32x32x3. Output = 28x28x6.\n",
        "    w1 = tf.Variable(tf.truncated_normal([5, 5, 3, 6], mean=mu, stddev=sigma))\n",
        "    b1 = tf.Variable(tf.zeros(6))\n",
        "    strides = [1, 1, 1, 1]\n",
        "    padding = 'VALID'\n",
        "    h1 = tf.nn.conv2d(x, w1, strides, padding) + b1\n",
        "\n",
        "    # Activation.\n",
        "    a1 = tf.nn.dropout(tf.nn.relu(h1), keep_prob)\n",
        "\n",
        "    # Pooling. Input = 28x28x6. Output = 14x14x6.\n",
        "    ksize = [1, 2, 2, 1]\n",
        "    strides = [1, 2, 2, 1]\n",
        "    padding = 'SAME'\n",
        "    a2 = tf.nn.dropout(tf.nn.max_pool(a1, ksize, strides, padding), keep_prob)\n",
        "    \n",
        "\n",
        "    # Layer 2: Convolutional. Output = 10x10x16.\n",
        "    w3 = tf.Variable(tf.truncated_normal([5, 5, 6, 16], mean=mu, stddev=sigma))\n",
        "    b3 = tf.Variable(tf.zeros(16))\n",
        "    strides = [1, 1, 1, 1]\n",
        "    padding = 'VALID'\n",
        "    h3 = tf.nn.conv2d(a2, w3, strides, padding) + b3\n",
        "\n",
        "    # Activation.\n",
        "    a3 = tf.nn.dropout(tf.nn.relu(h3), keep_prob)\n",
        "\n",
        "    # Pooling. Input = 10x10x16. Output = 5x5x16.\n",
        "    ksize = [1, 2, 2, 1]\n",
        "    strides = [1, 2, 2, 1]\n",
        "    padding = 'SAME'\n",
        "    a4 = tf.nn.dropout(tf.nn.max_pool(a3, ksize, strides, padding), keep_prob)\n",
        "\n",
        "    # Flatten. Input = 5x5x16. Output = 400.\n",
        "    a5 = tf.layers.flatten(a4)\n",
        "\n",
        "    # Layer 3: Fully Connected. Input = 400. Output = 120.\n",
        "    w6 = tf.Variable(tf.truncated_normal([400, 120], mean=mu, stddev=sigma))\n",
        "    b6 = tf.Variable(tf.zeros(120))\n",
        "    h6 = tf.add(tf.matmul(a5, w6), b6)\n",
        "\n",
        "    # Activation.\n",
        "    a6 = tf.nn.dropout(tf.nn.relu(h6), keep_prob)\n",
        "\n",
        "    # Layer 4: Fully Connected. Input = 120. Output = 84.\n",
        "    w7 = tf.Variable(tf.truncated_normal([120, 84], mean=mu, stddev=sigma))\n",
        "    b7 = tf.Variable(tf.zeros(84))\n",
        "    h7 = tf.add(tf.matmul(a6, w7), b7)\n",
        "\n",
        "    # Activation.\n",
        "    a7 = tf.nn.dropout(tf.nn.relu(h7), keep_prob)\n",
        "\n",
        "    # Layer 5: Fully Connected. Input = 84. Output = 43.\n",
        "    w8 = tf.Variable(tf.truncated_normal([84, 43], mean=mu, stddev=sigma))\n",
        "    b8 = tf.Variable(tf.zeros(43))\n",
        "\n",
        "    return tf.add(tf.matmul(a7, w8), b8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1p1nWvag9Zp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
        "y = tf.placeholder(tf.int32, (None))\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "one_hot_y = tf.one_hot(y, 43)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQxFpEyrioCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 50\n",
        "BATCH_SIZE = 128\n",
        "rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6T9M1kFip1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "logits = forward_pass(x)\n",
        "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
        "loss_operation = tf.reduce_mean(cross_entropy)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=rate)\n",
        "training_operation = optimizer.minimize(loss_operation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVdXAb8FisYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = tf.argmax(logits, 1)\n",
        "correct_prediction = tf.equal(prediction, tf.argmax(one_hot_y, 1))\n",
        "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "saver = tf.train.Saver()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Uulo0yNiyCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def evaluate(X_data, y_data):\n",
        "    \"\"\"Calculate the accuracy of the model\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X_data : Tensor\n",
        "        the input data\n",
        "    y_data : Tensor\n",
        "        the labels for the input data\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        the accuracy of the model\n",
        "    \n",
        "    \"\"\"\n",
        "    num_examples = len(X_data)\n",
        "    total_accuracy = 0\n",
        "    sess = tf.get_default_session()\n",
        "    for offset in range(0, num_examples, BATCH_SIZE):\n",
        "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
        "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})\n",
        "        total_accuracy += (accuracy * len(batch_x))\n",
        "    return total_accuracy / num_examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmESAt-mi2UL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    num_examples = len(X_train)\n",
        "\n",
        "    print(\"Training...\")\n",
        "    print()\n",
        "    for i in range(EPOCHS):\n",
        "        X_train, y_train = shuffle(X_train, y_train)\n",
        "        for offset in range(0, num_examples, BATCH_SIZE):\n",
        "            end = offset + BATCH_SIZE\n",
        "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
        "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 0.9})\n",
        "\n",
        "        validation_accuracy = evaluate(X_valid, y_valid)\n",
        "        print(\"EPOCH {} ...\".format(i+1))\n",
        "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
        "        print()\n",
        "\n",
        "    saver.save(sess, './traffic-signs')\n",
        "    print(\"Model saved\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRaFuDeni52h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}