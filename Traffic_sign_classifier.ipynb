{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67B6AzKicn8w",
        "colab_type": "text"
      },
      "source": [
        "All the necessary imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRUJQLVnan00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "from google.colab import files\n",
        "import io\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_6CEoB-djsr",
        "colab_type": "text"
      },
      "source": [
        "Fetching the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ww9l2K-lauZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(training_data_file, testing_data_file):\n",
        "    with open(training_data_file, mode='rb') as f:\n",
        "        train_data = pickle.load(f)\n",
        "    with open(testing_data_file, mode='rb') as f:\n",
        "        test_data = pickle.load(f)\n",
        "    return train, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3I-pEbIeuT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "data_path=\"/content/drive/My Drive/traffic_sign_classifier_data/\"\n",
        "\n",
        "training_data_file= open(data_path+'train.p','rb')\n",
        "test_data_file= open(data_path+'test.p','rb')\n",
        "valid_data_file=open(data_path+'valid.p', 'rb')\n",
        "\n",
        "train_data = pickle.load(training_data_file)\n",
        "test_data = pickle.load(test_data_file)\n",
        "valid_data = pickle.load(valid_data_file)\n",
        "\n",
        "X_train, y_train = train_data['features'], train_data['labels']\n",
        "X_test, y_test = test_data['features'], test_data['labels']\n",
        "X_valid,y_valid = valid_data['features'], valid_data['labels']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMleizhllBDO",
        "colab_type": "text"
      },
      "source": [
        "Pickled data is a dict having 4 key-values paris, namely\n",
        "features, labels, size and coords\n",
        "\n",
        "problem with coords --- it considers original image and pickled data has 32*32 downsampled image (this ate my head all the way)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtNmF4LSr9Zb",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Getting to know the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEjaKHq_iJMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of training examples\n",
        "n_train = len(X_train)\n",
        "\n",
        "# Number of validation examples\n",
        "n_validation = len(X_valid)\n",
        "\n",
        "# Number of testing examples.\n",
        "n_test = len(X_test)\n",
        "\n",
        "# What's the shape of an traffic sign image?\n",
        "image_shape = np.shape(X_train[0])\n",
        "\n",
        "# How many unique classes/labels there are in the dataset.\n",
        "n_classes = len(np.unique(y_train))\n",
        "\n",
        "print(\"Number of training examples =\", n_train)\n",
        "print(\"Number of testing examples =\", n_test)\n",
        "print(\"Number of validation examples =\", n_validation)\n",
        "print(\"Image data shape =\", image_shape)\n",
        "print(\"Number of classes =\", n_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50a_TF6c1Xl5",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Visualize some samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyMFl28g1W64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# show image of 20 random data points\n",
        "figs, axs = plt.subplots(4, 5, figsize = (16, 8))\n",
        "axs = axs.ravel()\n",
        "for i in range(20):\n",
        "    index = random.randint(0, len(X_train))\n",
        "    image = X_train[index]\n",
        "    axs[i].axis('off')\n",
        "    axs[i].imshow(image)\n",
        "    axs[i].set_title(y_train[index])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwzQZ1-1ylJ0",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Preprocessing the data\n",
        "doing the normalization (image centering to zero image and dividing by std_div)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t2hGpWCuF1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_preprocess(X):\n",
        "\n",
        "\n",
        "    X = np.float32(X)\n",
        "    \n",
        "    # standardize features\n",
        "    X -= np.mean(X)\n",
        "    X /= (np.std(X) + np.finfo('float32').eps)\n",
        "\n",
        "    # gray-scale conversion\n",
        "    X = np.sum(X/3, axis = 2, keepdims = True)\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "X_train = np.array([data_preprocess(img) for img in X_train])\n",
        "\n",
        "X_test = np.array([data_preprocess(img) for img in X_test])\n",
        "\n",
        "X_valid = np.array([data_preprocess(img) for img in X_valid])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWvwkGVOe7Zh",
        "colab_type": "text"
      },
      "source": [
        "Defining model architecture\n",
        "LeNet 5 which is taught in Udacity course is used here\n",
        "paper of a model related to same problem is at http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-oBg64Gg8Qy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LeNet(x):    \n",
        "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
        "    mu = 0\n",
        "    sigma = 0.1\n",
        "    \n",
        "    # Layer 1: Convolution. Input = 32x32x1. Output = 28x28x6.\n",
        "    conv1_w = tf.Variable(tf.truncated_normal(shape = (5, 5, 1, 6), mean = mu, stddev = sigma))\n",
        "    conv1_b = tf.Variable(tf.zeros(6))\n",
        "    conv1 = tf.nn.conv2d(x, conv1_w, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
        "    \n",
        "    # Activation\n",
        "    conv1 = tf.nn.relu(conv1)\n",
        "                          \n",
        "    # Pooling. Input = 28x28x6. Output = 14x14x6.\n",
        "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "                   \n",
        "    # Layer 2: Convolution. Output = 10x10x16.\n",
        "    conv2_w = tf.Variable(tf.truncated_normal([5, 5, 6, 16], mean = mu, stddev = sigma))\n",
        "    conv2_b = tf.Variable(tf.zeros(16))\n",
        "    conv2 = tf.nn.conv2d(conv1, conv2_w, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
        "\n",
        "    # Activation\n",
        "    conv2 = tf.nn.relu(conv2)\n",
        "    \n",
        "    # Pooling. Input = 10x10x16. Output = 5x5x16.\n",
        "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "\n",
        "    # Flatten. Input = 5x5x16. Output = 400.\n",
        "    conv2 = tf.layers.flatten(conv2)\n",
        "    \n",
        "    # Dropout layer 1\n",
        "    conv2 = tf.nn.dropout(conv2, 0.80)\n",
        "    \n",
        "    # Layer 3: Fully Connected. Input = 400. Output = 120.\n",
        "    fconnected1_w = tf.Variable(tf.truncated_normal([400, 120], mean = mu, stddev = sigma))\n",
        "    fconnected1_b = tf.Variable(tf.zeros(120))\n",
        "    fconnected_1 = tf.add(tf.matmul(conv2, fconnected1_w), fconnected1_b)\n",
        "    \n",
        "    # Activation.\n",
        "    fconnected_1 = tf.nn.relu(fconnected_1)\n",
        "    \n",
        "    # Dropout layer 2\n",
        "    fconnected_1 = tf.nn.dropout(fconnected_1, 0.70)                             \n",
        "    \n",
        "    # Layer 4: Fully Connected. Input = 120. Output = 84.\n",
        "    fconnected2_w = tf.Variable(tf.truncated_normal([120, 84], mean = mu, stddev = sigma))\n",
        "    fconnected2_b = tf.Variable(tf.zeros(84))\n",
        "    fconnected_2 = tf.add(tf.matmul(fconnected_1, fconnected2_w), fconnected2_b)\n",
        "    \n",
        "    # Activation.\n",
        "    fconnected_2 = tf.nn.relu(fconnected_2)\n",
        "    \n",
        "    # Layer 5: Fully Connected. Input = 84. Output = 43.\n",
        "    fconnected3_w = tf.Variable(tf.truncated_normal([84, 43], mean = mu, stddev = sigma))\n",
        "    fconnected3_b = tf.Variable(tf.zeros(43))\n",
        "    logits = tf.add(tf.matmul(fconnected_2, fconnected3_w), fconnected3_b)\n",
        "    \n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1p1nWvag9Zp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
        "y = tf.placeholder(tf.int32, (None))\n",
        "one_hot_y = tf.one_hot(y, 43)\n",
        "rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQxFpEyrioCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 50\n",
        "BATCH_SIZE = 128\n",
        "rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6T9M1kFip1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logits = LeNet(x)\n",
        "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
        "loss_operation = tf.reduce_mean(cross_entropy)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
        "training_operation = optimizer.minimize(loss_operation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVdXAb8FisYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
        "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Uulo0yNiyCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def evaluate(X_data, y_data):\n",
        "    \"\"\"Calculate the accuracy of the model\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X_data : Tensor\n",
        "        the input data\n",
        "    y_data : Tensor\n",
        "        the labels for the input data\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        the accuracy of the model\n",
        "    \n",
        "    \"\"\"\n",
        "    num_examples = len(X_data)\n",
        "    total_accuracy = 0\n",
        "    total_loss = 0\n",
        "    sess = tf.get_default_session()\n",
        "    # print(\"here...\")\n",
        "    # print()\n",
        "    for offset in range(0, num_examples, BATCH_SIZE):\n",
        "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
        "        loss, accuracy = sess.run([loss_operation, accuracy_operation], feed_dict={x: batch_x, y: batch_y})\n",
        "        total_accuracy += (accuracy * len(batch_x))\n",
        "        total_loss += (loss * len(batch_x))\n",
        "    return total_loss/float(num_examples), total_accuracy/float(num_examples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmESAt-mi2UL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss_history = []\n",
        "valid_loss_history = []\n",
        "\n",
        "train_accuracy_history = []\n",
        "valid_accuracy_history = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    num_examples = len(X_train)\n",
        "\n",
        "    print(\"Training...\")\n",
        "    print()\n",
        "    for i in range(EPOCHS):\n",
        "        X_train, y_train = shuffle(X_train, y_train)\n",
        "        for offset in range(0, num_examples, BATCH_SIZE):\n",
        "            end = offset + BATCH_SIZE\n",
        "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
        "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
        "\n",
        "        validation_loss, validation_accuracy = evaluate(X_valid, y_valid)\n",
        "        valid_loss_history.append(validation_loss)\n",
        "        valid_accuracy_history.append(validation_accuracy)\n",
        "        train_loss, train_accuracy = evaluate(X_train, y_train)\n",
        "        train_loss_history.append(train_loss)\n",
        "        train_accuracy_history.append(train_accuracy)\n",
        "        print(\"EPOCH {} ...\".format(i+1))\n",
        "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
        "        print(\"Training Accuracy = {:.3f}\".format(train_accuracy))\n",
        "        print(\"Validation loss = {:.3f}\".format(validation_loss))\n",
        "        print(\"Training loss = {:.3f}\".format(train_loss))\n",
        "        \n",
        "        print()\n",
        "\n",
        "    saver.save(sess, './traffic-signs')\n",
        "    print(\"Model saved\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLGWo4BxP3_d",
        "colab_type": "text"
      },
      "source": [
        "Plotting epochs vs training/validation loss and epochs vs training/validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRaFuDeni52h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "loss_plot = plt.subplot(2,1,1)\n",
        "loss_plot.set_title('Loss')\n",
        "loss_plot.plot(train_loss_history, 'r', label='Training Loss')\n",
        "loss_plot.plot(valid_loss_history, 'b', label='Validation Loss')\n",
        "loss_plot.set_xlim([0, EPOCHS])\n",
        "loss_plot.legend(loc = 1)\n",
        "\n",
        "plt.figure()\n",
        "accuracy_plot = plt.subplot(2,1,1)\n",
        "accuracy_plot.set_title('Accuracy')\n",
        "accuracy_plot.plot(train_accuracy_history, 'r', label='Training Accuracy')\n",
        "accuracy_plot.plot(valid_accuracy_history, 'b', label='Validation Accuracy')\n",
        "accuracy_plot.set_xlim([0, EPOCHS])\n",
        "accuracy_plot.legend(loc = 4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPFCof7FcnqC",
        "colab_type": "text"
      },
      "source": [
        "#TODO: Run over test dataset, try to reduce overfitting, report exponential average over validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikP2GF5lRWpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGSbfWiBdV5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}